{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Regression is **supervised** machine learning technique for predicting a **continuous** target variable. \n",
    "\n",
    "1. Ordinary Least Squares: [sklearn.linear_model.LinearRegression(Normalize=True)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "\n",
    "2. LASSO$^{1}$ + LARS$^{2}$  Performs both feature selection and noise reduction to avoid overfitting (through Regularization$^{3}$) to improve prediction performance and interpretability. Y should be normally distributed. [sklearn.linear_model.LassoLars(alpha=1.0)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html). \\$alpha$ = 0 is the OLS algorithm, so similar to running LinearRegression. Higher alpha will be more robust to collinearity between features. \n",
    "\n",
    "\n",
    "3. Polynomial Regression: just like an ordinary linear model, but where the features are polynomial. So we create polynomial features using [sklearn.preprocessing.PolynomialFeatures(degree=d)](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.fit_transform) and then fit a model using [sklearn.linear_model.LinearRegression(Normalize=True)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "\n",
    "4. Generalized Linear Model: Allows for different distributions, beyond just the Normal Distribution for OLS (and other models based on OLS, like LASSO).  [sklearn.linear_model.TweedieRegressor(power=n, alpha=1.0)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html)\n",
    "\n",
    "Awesome...but what do I do with that information? \n",
    "\n",
    "\n",
    "- For a normally distributed y and a linear relationship, the first 2 options are best. \n",
    "\n",
    "- For polynomial relationships, polynomial regression is best. \n",
    "\n",
    "- For normal, poisson, gamma or inverse gaussian distributions, use the Generalize Linear Model. \n",
    "\n",
    "\n",
    "How do you know what you have? \n",
    "\n",
    "1. plt.hist(y_train)\n",
    "\n",
    "2. what shape does it resemble?\n",
    "\n",
    "3. try different algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### for practice: \n",
    "\n",
    "1. set baseline predictions (mean, median)\n",
    "\n",
    "2. evaluate the baseline (we are comparing y (actual values) to the predicted values, which are all the same value...the mean of y, e.g.)\n",
    "\n",
    "    - y: 19, 18, 12, 8, 5\n",
    "    \n",
    "    - y_pred: 11, 11, 11, 11, 11\n",
    "\n",
    "3. LinearRegression()\n",
    "\n",
    "4. LassoLars()\n",
    "\n",
    "5. PolynomialFeatures(degree=2) ... then LinearRegression()\n",
    "\n",
    "for each one, evaluate with training predictions, and then with validate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path='student-mat.csv'\n",
    "\n",
    "df, X_train_explore, \\\n",
    "    X_train_scaled, y_train, \\\n",
    "    X_validate_scaled, y_validate, \\\n",
    "    X_test_scaled, y_test = wrangle.wrangle_student_math(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  \\\n",
       "142  0.000000  1.00  1.00    0.000000   0.666667  0.000000    0.75      0.25   \n",
       "326  0.333333  0.75  0.75    0.000000   0.000000  0.000000    0.75      0.50   \n",
       "88   0.166667  0.50  0.50    0.333333   0.333333  0.333333    0.75      0.75   \n",
       "118  0.333333  0.25  0.75    0.666667   0.333333  0.333333    1.00      0.25   \n",
       "312  0.666667  0.25  0.50    0.000000   0.333333  0.333333    0.75      1.00   \n",
       "\n",
       "     goout  Dalc  Walc  health  absences        G1        G2  \n",
       "142   0.25  0.00  0.00    1.00  0.035714  0.357143  0.578947  \n",
       "326   1.00  0.50  1.00    1.00  0.053571  0.714286  0.789474  \n",
       "88    0.25  0.00  0.00    0.50  0.214286  0.500000  0.526316  \n",
       "118   0.75  0.00  0.75    1.00  0.357143  0.357143  0.368421  \n",
       "312   0.25  0.25  0.25    0.75  0.053571  0.642857  0.578947  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20.,  0.,  1., 17., 34., 60., 30., 36., 14.,  9.]),\n",
       " array([ 0. ,  1.9,  3.8,  5.7,  7.6,  9.5, 11.4, 13.3, 15.2, 17.1, 19. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAD45JREFUeJzt3W2MXGd5xvH/RZyUFiix8SZ1E8yS1qKkHxKiVRSaFlGM0kAQdiuCglBrgSULFRCorYpbJEqrfkhalZdWqJVLAm6VgtNAaouEF8sEoX7AYIckJDjgJDLBjbENJARUqdRw98Mc09Wy65mdndlZP/x/0ui8Pcfn1rNnL599Zs6ZVBWSpLPf0yZdgCRpNAx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiNWLefB1q5dW9PT08t5SEk66x08ePDbVTXVr92yBvr09DQHDhxYzkNK0lkvyTcGaeeQiyQ1wkCXpEYY6JLUCANdkhphoEtSIwYK9CTnJ7k9yUNJDiV5cZI1SfYmOdxNV4+7WEnSwga9Qn8/8Kmq+jXgMuAQsB3YV1UbgH3dsiRpQvoGepJfBF4C3AxQVT+sqieBTcDOrtlOYPO4ipQk9TfIFfolwEngQ0m+nOSDSZ4BXFhVxwC66QVjrFOS1Mcgd4quAq4A3lpV+5O8n0UMryTZBmwDWL9+/VBFSuM2vf3OiR37yI3XTezYassgV+hHgaNVtb9bvp1ewB9Psg6gm56Yb+eq2lFVM1U1MzXV91EEkqQh9Q30qvoW8M0kL+hWbQS+CuwBtnTrtgC7x1KhJGkggz6c663ArUnOAx4F3kDvP4PbkmwFHgOuH0+JkqRBDBToVXUvMDPPpo2jLUeSNCzvFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIVYM0SnIE+D7wI+BUVc0kWQPsAqaBI8Brq+qJ8ZQpSepnMVfov11Vl1fVTLe8HdhXVRuAfd2yJGlCljLksgnY2c3vBDYvvRxJ0rAGDfQCPpPkYJJt3boLq+oYQDe9YL4dk2xLciDJgZMnTy69YknSvAYaQweurqrHk1wA7E3y0KAHqKodwA6AmZmZGqJGSdIABrpCr6rHu+kJ4A7gSuB4knUA3fTEuIqUJPXXN9CTPCPJs07PA9cADwB7gC1dsy3A7nEVKUnqb5AhlwuBO5Kcbv9vVfWpJF8CbkuyFXgMuH58ZUqS+ukb6FX1KHDZPOu/A2wcR1GSpMXzTlFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasTAgZ7knCRfTvKJbvn5SfYnOZxkV5LzxlemJKmfxVyhvw04NGv5JuC9VbUBeALYOsrCJEmLM1CgJ7kYuA74YLcc4GXA7V2TncDmcRQoSRrMoFfo7wP+FPhxt/wc4MmqOtUtHwUuGnFtkqRF6BvoSV4FnKiqg7NXz9O0Fth/W5IDSQ6cPHlyyDIlSf0McoV+NfDqJEeAj9IbankfcH6SVV2bi4HH59u5qnZU1UxVzUxNTY2gZEnSfPoGelX9WVVdXFXTwA3AZ6vq9cDdwGu6ZluA3WOrUpLU11I+h/4O4I+SPExvTP3m0ZQkSRrGqv5N/l9VfQ74XDf/KHDl6EuSJA3DO0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRi7pTVFI7prffObFjH7nxuokdu2VeoUtSIwx0SWqEgS5JjTDQJakRBrokNcJPuWhFmeQnL6SznVfoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhrRN9CTPD3JF5Pcl+TBJH/ZrX9+kv1JDifZleS88ZcrSVrIIFfo/wO8rKouAy4Hrk1yFXAT8N6q2gA8AWwdX5mSpH76Bnr1/KBbPLd7FfAy4PZu/U5g81gqlCQNZKCnLSY5BzgI/CrwAeAR4MmqOtU1OQpctMC+24BtAOvXr19qvVJzfMKkRmWgN0Wr6kdVdTlwMXAl8ML5mi2w746qmqmqmampqeErlSSd0aI+5VJVTwKfA64Czk9y+gr/YuDx0ZYmSVqMQT7lMpXk/G7+54GXA4eAu4HXdM22ALvHVaQkqb9BxtDXATu7cfSnAbdV1SeSfBX4aJK/Br4M3DzGOiVJffQN9Kq6H3jRPOsfpTeeLklaAbxTVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIa0TfQkzw3yd1JDiV5MMnbuvVrkuxNcribrh5/uZKkhQxyhX4K+OOqeiFwFfDmJJcC24F9VbUB2NctS5ImpG+gV9Wxqrqnm/8+cAi4CNgE7Oya7QQ2j6tISVJ/ixpDTzINvAjYD1xYVcegF/rABaMuTpI0uFWDNkzyTOBjwNur6qkkg+63DdgGsH79+mFqBGB6+51D77sUR268biLHlaTFGugKPcm59ML81qr6eLf6eJJ13fZ1wIn59q2qHVU1U1UzU1NTo6hZkjSPQT7lEuBm4FBVvWfWpj3Alm5+C7B79OVJkgY1yJDL1cDvA19Jcm+37s+BG4HbkmwFHgOuH0+JkqRB9A30qvpPYKEB842jLUeSNCzvFJWkRhjoktQIA12SGmGgS1IjDHRJasTAd4rqZ8ek7sqVtDReoUtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiO8U1TSsvM7gsfDK3RJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5Jjegb6EluSXIiyQOz1q1JsjfJ4W66erxlSpL6GeQK/cPAtXPWbQf2VdUGYF+3LEmaoL6BXlWfB747Z/UmYGc3vxPYPOK6JEmLNOwY+oVVdQygm16wUMMk25IcSHLg5MmTQx5OktTP2N8UraodVTVTVTNTU1PjPpwk/cwaNtCPJ1kH0E1PjK4kSdIwhg30PcCWbn4LsHs05UiShtX3G4uSfAR4KbA2yVHgL4AbgduSbAUeA64fZ5GSNAqtf1NS30CvqtctsGnjiGuRJC2Bd4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiNWLWXnJNcC7wfOAT5YVTeOpCoBML39zkmXIOksMvQVepJzgA8ArwAuBV6X5NJRFSZJWpylDLlcCTxcVY9W1Q+BjwKbRlOWJGmxlhLoFwHfnLV8tFsnSZqApYyhZ5519VONkm3Atm7xB0m+NuTx1gLfHnLfoeWmgZtOpL5FsL6lsb6l+ZmubxE5spDnDdJoKYF+FHjurOWLgcfnNqqqHcCOJRwHgCQHqmpmqf/OuFjf0ljf0ljf0qz0+ga1lCGXLwEbkjw/yXnADcCe0ZQlSVqsoa/Qq+pUkrcAn6b3scVbqurBkVUmSVqUJX0OvaruAu4aUS39LHnYZsysb2msb2msb2lWen0DSdVPvY8pSToLeeu/JDVixQV6kmuTfC3Jw0m2z7P955Ls6rbvTzK9jLU9N8ndSQ4leTDJ2+Zp89Ik30tyb/d613LV1x3/SJKvdMc+MM/2JPn7rv/uT3LFMtb2gln9cm+Sp5K8fU6bZe2/JLckOZHkgVnr1iTZm+RwN129wL5bujaHk2xZxvr+NslD3c/vjiTnL7DvGc+FMdb37iT/Netn+MoF9j3j7/oY69s1q7YjSe5dYN+x99/IVdWKedF7c/UR4BLgPOA+4NI5bf4Q+Kdu/gZg1zLWtw64opt/FvD1eep7KfCJCfbhEWDtGba/EvgkvfsIrgL2T/Bn/S3geZPsP+AlwBXAA7PW/Q2wvZvfDtw0z35rgEe76epufvUy1XcNsKqbv2m++gY5F8ZY37uBPxng53/G3/Vx1Tdn+98B75pU/436tdKu0Ad5nMAmYGc3fzuwMcl8NzmNXFUdq6p7uvnvA4c4++6O3QT8S/V8ATg/yboJ1LEReKSqvjGBY/9EVX0e+O6c1bPPsZ3A5nl2/R1gb1V9t6qeAPYC1y5HfVX1mao61S1+gd49IBOxQP8NYlkeHXKm+rrceC3wkVEfd1JWWqAP8jiBn7TpTurvAc9Zlupm6YZ6XgTsn2fzi5Pcl+STSX59WQvr3a37mSQHu7t051opj2y4gYV/kSbZfwAXVtUx6P0nDlwwT5uV0o9vpPcX13z6nQvj9JZuSOiWBYasVkL//RZwvKoOL7B9kv03lJUW6IM8TmCgRw6MU5JnAh8D3l5VT83ZfA+9YYTLgH8A/mM5awOurqor6D0F881JXjJn+0rov/OAVwP/Ps/mSfffoFZCP74TOAXcukCTfufCuPwj8CvA5cAxesMac028/4DXcear80n139BWWqAP8jiBn7RJsgp4NsP9yTeUJOfSC/Nbq+rjc7dX1VNV9YNu/i7g3CRrl6u+qnq8m54A7qD3p+1sAz2yYcxeAdxTVcfnbph0/3WOnx6G6qYn5mkz0X7s3oR9FfD66gZ85xrgXBiLqjpeVT+qqh8D/7zAcSfdf6uA3wN2LdRmUv23FCst0Ad5nMAe4PQnCl4DfHahE3rUujG3m4FDVfWeBdr80ukx/SRX0uvj7yxTfc9I8qzT8/TePHtgTrM9wB90n3a5Cvje6eGFZbTgldEk+2+W2efYFmD3PG0+DVyTZHU3pHBNt27s0vtimXcAr66q/16gzSDnwrjqm/2ezO8ucNxJPzrk5cBDVXV0vo2T7L8lmfS7snNf9D6F8XV674C/s1v3V/ROXoCn0/tT/WHgi8Aly1jbb9L7s/B+4N7u9UrgTcCbujZvAR6k9679F4DfWMb6LumOe19Xw+n+m11f6H0xySPAV4CZZf75/gK9gH72rHUT6z96/7EcA/6X3lXjVnrvyewDDnfTNV3bGXrfzHV63zd25+HDwBuWsb6H6Y0/nz4HT3/q65eBu850LixTff/anVv30wvpdXPr65Z/6nd9Oerr1n/49Dk3q+2y99+oX94pKkmNWGlDLpKkIRnoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ14v8AJFiz2UA7j1kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoLars\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.524886877828054"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train)\n",
    "# np.median(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "baseline_rmse = mean_squared_error(y_train, np.full(221, np.mean(y_train)))**1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multiple models\n",
    "\n",
    "- LinearRegression()\n",
    "\n",
    "- LassoLars()\n",
    "\n",
    "- PolynomialFeatures(), LinearRegression()\n",
    "\n",
    "- TweedieRegressor()\n",
    "\n",
    "### LinearRegression()\n",
    "\n",
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression(normalize=True)\n",
    "lm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict our **training** observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_pred = lm.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our **training** predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5318707004095158"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute root mean squared error\n",
    "lm_rmse = mean_squared_error(y_train, lm_pred)**1/2\n",
    "lm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I find this is one of my top models:**\n",
    "\n",
    "Predict our **validation** observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_pred_v = lm.predict(X_validate_scaled)\n",
    "# lm_pred_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our **validation** predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.260805772673469"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_rmse_v = mean_squared_error(y_validate, lm_pred_v)**1/2\n",
    "lm_rmse_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoLars()\n",
    "\n",
    "Fit the model using **training** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoLars(alpha=0.1, copy_X=True, eps=2.220446049250313e-16,\n",
       "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
       "     positive=False, precompute='auto', verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars = LassoLars(alpha=0.1)\n",
    "lars.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict our **training** observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lars_pred = lars.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our **training** predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8706853377983266"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars_rmse = mean_squared_error(y_train, lars_pred)**1/2\n",
    "lars_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I find this is one of my top models:**\n",
    "\n",
    "Predict our **validation** observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lars_pred_v = lars.predict(X_validate_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our **validation** predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.091594747961"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars_rmse_v = mean_squared_error(y_validate, lars_pred_v)**1/2\n",
    "lars_rmse_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynomialFeatures + LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd degree polynomial\n",
    "\n",
    "1. Create the new features, which are our original features squared. \n",
    "2. fit the Linear Regression model\n",
    "3. Predict on train\n",
    "4. Evaluate on train\n",
    "5. transform our validate features to make them squared\n",
    "6. predict on validate\n",
    "7. evaluate on validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial thing\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform the thing\n",
    "# to get a new set of features..which are the original features sqauared\n",
    "X_train_squared = pf.fit_transform(X_train_scaled)\n",
    "X_validate_squared = pf.transform(X_validate_scaled)\n",
    "# X_validate_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed that data into our linear model. \n",
    "# make the thing\n",
    "lm_squared = LinearRegression()\n",
    "lm_squared.fit(X_train_squared, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict our training observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_squared_pred = lm_squared.predict(X_train_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our training predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5450147535110965"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_squared_rmse = mean_squared_error(y_train, lm_squared_pred)**1/2\n",
    "lm_squared_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I find this is one of my top models:**\n",
    "\n",
    "Predict our **validation** observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_squared_pred_v = lm_squared.predict(X_validate_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our **validation** predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.85621291143576"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_squared_rmse_v = mean_squared_error(y_validate, lm_squared_pred_v)**1/2\n",
    "lm_squared_rmse_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large discrepancy here indicates possible overfitting of the quadratic model to our training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a third degress polynomial\n",
    "\n",
    "create the new features, which are the original features, cubed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial thing\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# fit and transform the thing\n",
    "# to get a new set of features..which are the original features sqauared\n",
    "X_train_cubed = pf.fit_transform(X_train_scaled)\n",
    "X_validate_cubed = pf.transform(X_validate_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using our training observations that have been cubed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed that data into our linear model. \n",
    "# make the thing\n",
    "lm_cubed = LinearRegression()\n",
    "lm_cubed.fit(X_train_cubed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict our training observations (the cubed version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_cubed_pred = lm_cubed.predict(X_train_cubed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our predictions on training observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5891431053206286e-28"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_cubed_rmse = mean_squared_error(y_train, lm_cubed_pred)**1/2\n",
    "lm_cubed_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will this perform on data it has not seen? Has it overfit to the training data? \n",
    "\n",
    "Predict on the validation observations, but the dataset that we have cubed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_cubed_pred_v = lm_cubed.predict(X_validate_cubed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our predictions on validation observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.63132019137655"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_cubed_rmse_v = mean_squared_error(y_validate, lm_cubed_pred_v)**1/2\n",
    "lm_cubed_rmse_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the errors and the difference in errors from train to validate for each of the models, it appears an ordinary least squares is the best fit for our data. \n",
    "We will go with the lm model. \n",
    "Predict on test and evaluate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8489414051769122"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_pred_test = lm.predict(X_test_scaled)\n",
    "lm_rmse_test = mean_squared_error(y_test, lm_pred_test)**1/2\n",
    "lm_rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.120165434778158"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and compare\n",
    "\n",
    "- metrics.mean_squared_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline, Mean:  10.120165434778158\n",
      "Linear Model:  1.5318707004095158\n",
      "LassoLars:  2.8706853377983266\n",
      "Polynomial, squared:  0.5450147535110965\n",
      "Polynomial, cubed:  3.5891431053206286e-28\n"
     ]
    }
   ],
   "source": [
    "lm_rmse = mean_squared_error(y_train, lm_pred)**1/2\n",
    "lars_rmse = mean_squared_error(y_train, lars_pred)**1/2\n",
    "lm_squared_rmse = mean_squared_error(y_train, lm_squared_pred)**1/2\n",
    "lm_cubed_rmse = mean_squared_error(y_train, lm_cubed_pred)**1/2\n",
    "\n",
    "print(\"Baseline, Mean: \", baseline_rmse)\n",
    "print(\"Linear Model: \", lm_rmse)\n",
    "print(\"LassoLars: \", lars_rmse)\n",
    "print(\"Polynomial, squared: \", lm_squared_rmse)\n",
    "print(\"Polynomial, cubed: \", lm_cubed_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate: did we overfit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [95, 221]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-00db5b39e4be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlm_validate_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlm_rmse_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlm_rmse_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 239\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [95, 221]"
     ]
    }
   ],
   "source": [
    "lm_validate_pred = lm.predict(X_validate_scaled)\n",
    "lm_rmse_validate = mean_squared_error(y_validate, lm_pred)**1/2\n",
    "lm_rmse_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$^{1}$ LASSO = Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "$^{2}$ LARS = Least Angle Regression\n",
    "\n",
    "$^{3}$ Regularization = \"Regularizations are techniques used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting.\" [Towards Data Science](https://towardsdatascience.com/regularization-an-important-concept-in-machine-learning-5891628907ea)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
